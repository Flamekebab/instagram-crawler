#!/usr/bin/env python3
import csv
import sys
import json
import string
import requests
from requests.auth import HTTPBasicAuth 
from typing import List
import re
from instaclient import InstaClient
from instaclient.errors import *
#from datetime import datetime
#from dateutil.parser import parse
from proxycrawl import CrawlingAPI, ScraperAPI, LeadsAPI

#Python doesn't have PHP's var_dump. Fortunately someone wrote an equivalent so we have it here for debugging:
from var_dump import var_dump

# Create a instaclient object. Place as driver_path argument the path that leads to where you saved the chromedriver.exe file
client = InstaClient(driver_path='./chromedriver')

from instaclient.errors import *

# try:
#     client.login(username='', password='') # Go through Login Procedure
# except VerificationCodeNecessary:
#     # This error is raised if the user has 2FA turned on.
#     code = input('Enter the 2FA security code generated by your Authenticator App or sent to you by SMS')
#     client.input_verification_code(code)
# except SuspisciousLoginAttemptError as error:
#     # This error is reaised by Instagram
#     if error.mode == SuspisciousLoginAttemptError.EMAIL:
#         code = input('Enter the security code that was sent to you via email: ')
#     else:
#         code = input('Enter the security code that was sent to you via SMS: ')
#     client.input_security_code(code)

username = "username-to-crawl"

followersQuery = client.get_followers(user=username, count=50) # replace with the target username

followersRawData = followersQuery[0]

# each follower is one user and contains the following things we care about:
# username, full_name, is_private

# Initialise the list of dictionaries that we're going to end up with
followerList = []

# Work through the data grabbing only the relevant bits of info. Add them to a dictionary and append those to the list we're building.
for follower in followersRawData:
    followerInfo = {}
    followerInfo['username'] = follower.username
    followerInfo['full_name'] = follower.name
    followerInfo['is_private'] = follower.is_private
    followerList.append(followerInfo)

#var_dump(followerList)

#Let's output the data as JSON.
jsonPath = "./output.json"
with open(jsonPath, 'w') as outfile:
        json.dump(followerList, outfile)

# #Now let's get started with the ProxyCrawl scraper API
